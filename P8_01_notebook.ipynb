{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a virtual environment in Ubuntu 20.04.2 LTS\n",
    "\n",
    "Spark version: spark-2.4.7-bin-hadoop2.7\n",
    "\n",
    "Java 8 !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findspark : to use spark within a jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findspark needs the environment variable SPARK_HOME to work (indicate the spark directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/maryse/spark-2.4.7-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure SPARK_HOME is correctly set (in .bashrc)\n",
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the right path to java 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import findspark and initialize findspark (allow to use Spark with the notebook)\n",
    "\n",
    "Makes pyspark available in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the environnement variable 'PYSPARK_SUBMIT_ARGS' in order :\n",
    "- to fetch the databricks sparkdl package, as soon as the pyspark-submit command will be run\n",
    "- to make Hadoop AWS package available when spark will be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2,databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explore functions of a module\n",
    "# from inspect import getmembers, isfunction\n",
    "# print(pd.DataFrame(getmembers(pyspark.sql)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of AWS access keys from key file\n",
    "\n",
    "path_cred = os.path.join(os.getcwd(),\n",
    "            \"AWS/AWS_IAM_CREDENTIAL/Maryse_P8_credentials.csv\")\n",
    "\n",
    "with open(path_cred,'r') as f:\n",
    "        msg = f.read()\n",
    "          \n",
    "ID = str(msg).split('\\n')[1].split(',')[2]\n",
    "KEY = str(msg).split('\\n')[1].split(',')[3]\n",
    "\n",
    "# set \"temporary\" environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or use the configparser to read the credentials from our awsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configparser\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read(os.path.expanduser(\"AWS/AWS_IAM_CREDENTIAL\"))\n",
    "# access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "# access_key = config.get(aws_profile, \"aws_secret_access_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and set parameters of the Hadoop configuration in order to be able to fetch data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# conf = (SparkConf().set('spark.executor.extraJavaOptions',\n",
    "#                         '-Dcom.amazonaws.services.s3.enableV4=true')\\\n",
    "#          .set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'))\n",
    "\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate() #conf=conf)\n",
    "# sc.setSystemProperty('com.amazonaws.services.s3.enableV4',\n",
    "#                      'true')\n",
    "\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\",\n",
    "                \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3.awsAccessKeyId\", ID)\n",
    "hadoop_conf.set(\"fs.s3.awsSecretAccessKey\", KEY)\n",
    "\n",
    "# hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "# hadoopConf.set('fs.s3a.awsAccessKeyId', ID)\n",
    "# hadoopConf.set('fs.s3a.awsSecretAccessKey', KEY)\n",
    "# hadoopConf.set('fs.s3a.endpoint', 's3-us-east-2.amazonaws.com')\n",
    "# hadoopConf.set('com.amazonaws.services.s3a.enableV4', 'true')\n",
    "# hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of SparkContext and import sparkdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SparkContext become useless if SparkSession (spark.sql) is created\n",
    "# from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # En cas de pbe li√© aux serveurs S3 choisis\n",
    "# conf = (SparkConf().set('spark.executor.extraJavaOptions',\n",
    "#                         '-Dcom.amazonaws.services.s3.enableV4=true')\\\n",
    "#                    .set('spark.driver.extraJavaOptions',\n",
    "#                         '-Dcom.amazonaws.services.s3.enableV4=true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Default SparkContext\n",
    "# sc = SparkContext()\n",
    "\n",
    "# # # Custom SparkContext\n",
    "# # sc=SparkContext(conf=conf)\n",
    "# # sc.setSystemProperty('com.amazonaws.services.s3.enableV4',\n",
    "# #                      'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('FeatExtr').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then only we import sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# show only one warning if multiple warnings in the same cell\n",
    "warnings.filterwarnings(\"ignore\") # \"once\"\n",
    "\n",
    "import sparkdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame containing all the pictures\n",
    "\n",
    "### Read images and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads recursively all images in the specified directory, put in a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option1: Get local data\n",
    "\n",
    "data_path = \"./DATA/fruits-360/SAMPLE\" # /Corn\"\n",
    "    \n",
    "# Option2: Get data from s3\n",
    "\n",
    "# bucket='ocfruitpictures'\n",
    "# # folder = 'SAMPLE'\n",
    "# folder = 'Training'\n",
    "# data_path = 's3://{}/{}'.format(bucket, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads all images contained in the directory\n",
    "\n",
    "images_df = ImageSchema.readImages(data_path,\n",
    "                                   recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the content of the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a Spark DataFrame containing all the images, each as one row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample picture\n",
    "\n",
    "Extract first picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract first row of the DataFrame\n",
    "# row0 = images_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row0.asDict()['image']['mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform the row in a dict, and turn the data in a 1D np.array\n",
    "# mat0 = np.array(row0.asDict()['image']['data'])\n",
    "# # reshape the 1D vector into a 3 channel, 2D np.array of pixels\n",
    "# mat0 = mat0.reshape(100, 100, 3)[:, :, ::-1] # reverse BGR to RGB\n",
    "# mat0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# # Display sample image\n",
    "# Image.fromarray(mat0, 'RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction (Transfer Learning) using Sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import DeepImageFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of the featurizer\n",
    "feat = DeepImageFeaturizer(inputCol=\"image\",\n",
    "                           outputCol=\"image_features\",\n",
    "                           modelName=\"ResNet50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiation of a sparkdl pipeline to process the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe = Pipeline(stages=[feat])\n",
    "extractor = pipe.fit(images_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_features_df = extractor.transform(images_df)\n",
    "# ext_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare the size of the Spark DataFrame (prior action)\n",
    "# # and that of a Pandas DataFrame\n",
    "\n",
    "# import sys\n",
    "# print(sys.getsizeof(ext_features_df),\n",
    "#       sys.getsizeof(ext_features_df.toPandas()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# instantiate Spark PCA model\n",
    "pca = PCA(k=8,\n",
    "          inputCol=\"image_features\",\n",
    "          outputCol=\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the extracted features\n",
    "model = pca.fit(ext_features_df.select('image_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative explained variance\n",
    "cumValues = model.explainedVariance.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative explained variance')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show the scree plot\n",
    "plt.rcParams['figure.facecolor']='w'\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(range(1,9), cumValues, color='r',\n",
    "         marker = 'o', linestyle='--')\n",
    "plt.title('Scree plot')\n",
    "plt.xlabel('Number of first components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               image|      image_features|        pca_features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[file:/home/marys...|[0.24925154447555...|[-12.529673163011...|\n",
      "|[file:/home/marys...|[0.45899030566215...|[-11.801572304069...|\n",
      "|[file:/home/marys...|[2.24640989303588...|[-18.164272505059...|\n",
      "|[file:/home/marys...|[1.94920766353607...|[-13.197414654339...|\n",
      "|[file:/home/marys...|[0.53471016883850...|[-15.306634451404...|\n",
      "|[file:/home/marys...|[0.02559056505560...|[20.2904767252304...|\n",
      "|[file:/home/marys...|[0.10091863572597...|[17.3329560039164...|\n",
      "|[file:/home/marys...|[0.13858571648597...|[23.6065910954511...|\n",
      "|[file:/home/marys...|[0.08605482429265...|[9.20104508968864...|\n",
      "|[file:/home/marys...|[0.00671436730772...|[16.2469733598572...|\n",
      "|[file:/home/marys...|[0.06267129629850...|[8.94065403878746...|\n",
      "|[file:/home/marys...|[0.01391865313053...|[12.0976046936089...|\n",
      "|[file:/home/marys...|[0.03199945762753...|[6.05263226903837...|\n",
      "|[file:/home/marys...|[0.0,1.0963382720...|[9.93913323393617...|\n",
      "|[file:/home/marys...|[0.04331192746758...|[6.22109150352976...|\n",
      "|[file:/home/marys...|[0.05217304080724...|[-1.2703070985824...|\n",
      "|[file:/home/marys...|[0.0,0.0012494423...|[0.60333565937887...|\n",
      "|[file:/home/marys...|[0.55651140213012...|[-5.1119142155751...|\n",
      "|[file:/home/marys...|[0.10565294325351...|[-3.9754108642788...|\n",
      "|[file:/home/marys...|[0.10405083000659...|[-1.6098549850545...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the projection of the extracted features using PCA\n",
    "\n",
    "pca_feat_df = model.transform(ext_features_df)\n",
    "pca_feat_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the class of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|                path|      image_features|        pca_features|     labels|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|[file:/home/marys...|[0.24925154447555...|[-12.529673163011...| Clementine|\n",
      "|[file:/home/marys...|[0.45899030566215...|[-11.801572304069...| Clementine|\n",
      "|[file:/home/marys...|[2.24640989303588...|[-18.164272505059...| Clementine|\n",
      "|[file:/home/marys...|[1.94920766353607...|[-13.197414654339...| Clementine|\n",
      "|[file:/home/marys...|[0.53471016883850...|[-15.306634451404...| Clementine|\n",
      "|[file:/home/marys...|[0.02559056505560...|[20.2904767252304...|       Corn|\n",
      "|[file:/home/marys...|[0.10091863572597...|[17.3329560039164...|       Corn|\n",
      "|[file:/home/marys...|[0.13858571648597...|[23.6065910954511...|       Corn|\n",
      "|[file:/home/marys...|[0.08605482429265...|[9.20104508968864...|       Corn|\n",
      "|[file:/home/marys...|[0.00671436730772...|[16.2469733598572...|       Corn|\n",
      "|[file:/home/marys...|[0.06267129629850...|[8.94065403878746...|Ginger Root|\n",
      "|[file:/home/marys...|[0.01391865313053...|[12.0976046936089...|Ginger Root|\n",
      "|[file:/home/marys...|[0.03199945762753...|[6.05263226903837...|Ginger Root|\n",
      "|[file:/home/marys...|[0.0,1.0963382720...|[9.93913323393617...|Ginger Root|\n",
      "|[file:/home/marys...|[0.04331192746758...|[6.22109150352976...|Ginger Root|\n",
      "|[file:/home/marys...|[0.05217304080724...|[-1.2703070985824...|     Lychee|\n",
      "|[file:/home/marys...|[0.0,0.0012494423...|[0.60333565937887...|     Lychee|\n",
      "|[file:/home/marys...|[0.55651140213012...|[-5.1119142155751...|     Lychee|\n",
      "|[file:/home/marys...|[0.10565294325351...|[-3.9754108642788...|     Lychee|\n",
      "|[file:/home/marys...|[0.10405083000659...|[-1.6098549850545...|     Lychee|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Get class of the sample fruits\n",
    "\n",
    "orig_col = pca_feat_df['image']['origin']\n",
    "split_col = pyspark.sql.functions.split(orig_col,\n",
    "                                        'SAMPLE/')\n",
    "\n",
    "# add a new \"label\" column\n",
    "df_ = pca_feat_df.withColumn('labels',\n",
    "                                 split_col.getItem(1))\n",
    "split_col = pyspark.sql.functions.split(df_['labels'],\n",
    "                                        '/')\n",
    "df_ = df_.withColumn('labels',\n",
    "                     split_col.getItem(0))\n",
    "\n",
    "df_ = df_.withColumnRenamed(\"image\", \"path\")\n",
    "\n",
    "df_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = df_.select('path','pca_features','labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write final DataFrame in parquet format in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.csv as pv\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# # results_pd = results_df.toPandas()\n",
    "# pq.write_table(results_df, 'test1.parquet') # Index(['path', 'pca_features', 'labels'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\") # gzip, lzo or lz4\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"uncompressed\")\n",
    "# spark.sql(\"SET parquet.compression=SNAPPY\")\n",
    "# spark.sql(\"SET spark.sql.parquet.compression.codec=snappy\")\n",
    "# df_.write.parquet(\"p0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_pd = results_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o257.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/P2.parquet/_temporary/0/task_20210308175202_0009_m_000000/part-00000-4ef3db71-38ad-4f8e-9c29-d40ef46fd28d-c000.parquet (Aucun fichier ou dossier de ce type)\n\tat java.io.FileInputStream.open0(Native Method)\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:364)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:374)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:613)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:414)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:428)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:362)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:334)\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:166)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-db3654373e25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"file:///home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/P2.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o257.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/P2.parquet/_temporary/0/task_20210308175202_0009_m_000000/part-00000-4ef3db71-38ad-4f8e-9c29-d40ef46fd28d-c000.parquet (Aucun fichier ou dossier de ce type)\n\tat java.io.FileInputStream.open0(Native Method)\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:364)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:374)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:613)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:414)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:428)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:362)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:334)\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:166)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "path = \"file:///home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/P2.parquet\"\n",
    "results_df.write.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/P0.json\"\n",
    "# results_df.write.json('truc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.write.parquet(\"s3://ocfruitpictures/RESULTS_all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
