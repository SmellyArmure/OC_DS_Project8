{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2,databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cred = os.path.join(os.getcwd(),\n",
    "            \"AWS/AWS_IAM_CREDENTIAL/Maryse_P8_credentials.csv\")\n",
    "\n",
    "with open(path_cred,'r') as f:\n",
    "        msg = f.read()\n",
    "          \n",
    "ID = str(msg).split('\\n')[1].split(',')[2]\n",
    "KEY = str(msg).split('\\n')[1].split(',')[3]\n",
    "\n",
    "# set \"temporary\" environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\",\n",
    "                \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3.awsAccessKeyId\", ID)\n",
    "hadoop_conf.set(\"fs.s3.awsSecretAccessKey\", KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of SparkContext and import sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FeatExtr').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maryse/anaconda3/envs/P8_venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/maryse/anaconda3/envs/P8_venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/maryse/anaconda3/envs/P8_venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/maryse/anaconda3/envs/P8_venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/maryse/anaconda3/envs/P8_venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/maryse/anaconda3/envs/P8_venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sparkdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame containing all the pictures\n",
    "\n",
    "### Read images and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'SAMPLE'\n",
    "\n",
    "# # Option1: Get local data\n",
    "# data_path = os.path.join(\"./DATA/fruits-360\", PREFIX)\n",
    "    \n",
    "# Option2: Get data from s3\n",
    "bucket='ocfruitpictures'\n",
    "data_path = 's3://{}/{}'.format(bucket, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = ImageSchema.readImages(data_path,\n",
    "                                   recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction (Transfer Learning) using Sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import DeepImageFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = DeepImageFeaturizer(inputCol=\"image\",\n",
    "                           outputCol=\"image_features\",\n",
    "                           modelName=\"ResNet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe = Pipeline(stages=[feat])\n",
    "extractor = pipe.fit(images_df)\n",
    "ext_features_df = extractor.transform(images_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=8,\n",
    "          inputCol=\"image_features\",\n",
    "          outputCol=\"pca_features\")\n",
    "model = pca.fit(ext_features_df.select('image_features'))\n",
    "pca_feat_df = model.transform(ext_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the class of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as pspfunc\n",
    "\n",
    "orig_col = pca_feat_df['image']['origin']\n",
    "split_col = pspfunc.split(orig_col, PREFIX+'/')\n",
    "df_ = pca_feat_df.withColumn('labels', split_col.getItem(1))\n",
    "split_col = pspfunc.split(df_['labels'], '/')\n",
    "df_ = df_.withColumn('labels', split_col.getItem(0))\n",
    "df_ = df_.withColumnRenamed(\"image\", \"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = df_.select('path','pca_features','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.write.mode('overwrite').parquet(\"s3://ocfruitpictures/RESULTS_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path = \"file:///home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/Pultimate.parquet\"\n",
    "# path = \"Pult.parquet\"\n",
    "# # path = \"/home/maryse/Pult.parquet\"\n",
    "# results_df.write.mode('overwrite').parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
