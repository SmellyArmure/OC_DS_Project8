{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a virtual environment in Ubuntu 20.04.2 LTS\n",
    "\n",
    "Spark version: spark-2.4.7-bin-hadoop2.7\n",
    "\n",
    "Java 8 !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findspark : to use spark within a jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findspark needs the environment variable SPARK_HOME to work (indicate the spark directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure SPARK_HOME is correctly set (in .bashrc)\n",
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the right path to java 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import findspark and initialize findspark (allow to use Spark with the notebook)\n",
    "\n",
    "Makes pyspark available in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the environnement variable 'PYSPARK_SUBMIT_ARGS' in order :\n",
    "- to fetch the databricks sparkdl package, as soon as the pyspark-submit command will be run\n",
    "- to make Hadoop AWS package available when spark will be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2,databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explore functions of a module\n",
    "# from inspect import getmembers, isfunction\n",
    "# print(pd.DataFrame(getmembers(pyspark.sql)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of AWS access keys from key file\n",
    "\n",
    "path_cred = os.path.join(\"/home/maryse/p8/AWS/AWS_IAM_CREDENTIAL/Maryse_P8_credentials.csv\")\n",
    "\n",
    "with open(path_cred,'r') as f:\n",
    "        msg = f.read()\n",
    "          \n",
    "ID = str(msg).split('\\n')[1].split(',')[2]\n",
    "KEY = str(msg).split('\\n')[1].split(',')[3]\n",
    "\n",
    "# set \"temporary\" environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or use the configparser to read the credentials from our awsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configparser\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read(os.path.expanduser(\"AWS/AWS_IAM_CREDENTIAL\"))\n",
    "# access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "# access_key = config.get(aws_profile, \"aws_secret_access_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and set parameters of the Hadoop configuration in order to be able to fetch data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# conf = (SparkConf().set('spark.executor.extraJavaOptions',\n",
    "#                         '-Dcom.amazonaws.services.s3.enableV4=true')\\\n",
    "#          .set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'))\n",
    "\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate() #conf=conf)\n",
    "# sc.setSystemProperty('com.amazonaws.services.s3.enableV4',\n",
    "#                      'true')\n",
    "\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3n.impl\",\n",
    "                \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3n.awsAccessKeyId\", ID)\n",
    "hadoop_conf.set(\"fs.s3n.awsSecretAccessKey\", KEY)\n",
    "\n",
    "# hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "# hadoopConf.set('fs.s3a.awsAccessKeyId', ID)\n",
    "# hadoopConf.set('fs.s3a.awsSecretAccessKey', KEY)\n",
    "# hadoopConf.set('fs.s3a.endpoint', 's3-us-east-2.amazonaws.com')\n",
    "# hadoopConf.set('com.amazonaws.services.s3a.enableV4', 'true')\n",
    "# hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of SparkContext and import sparkdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SparkContext become useless if SparkSession (spark.sql) is created\n",
    "# from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # En cas de pbe li√© aux serveurs S3 choisis\n",
    "# conf = (SparkConf().set('spark.executor.extraJavaOptions',\n",
    "#                         '-Dcom.amazonaws.services.s3.enableV4=true')\\\n",
    "#                    .set('spark.driver.extraJavaOptions',\n",
    "#                         '-Dcom.amazonaws.services.s3.enableV4=true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Default SparkContext\n",
    "# sc = SparkContext()\n",
    "\n",
    "# # # Custom SparkContext\n",
    "# # sc=SparkContext(conf=conf)\n",
    "# # sc.setSystemProperty('com.amazonaws.services.s3.enableV4',\n",
    "# #                      'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('FeatExtr').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then only we import sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# show only one warning if multiple warnings in the same cell\n",
    "warnings.filterwarnings(\"ignore\") # \"once\"\n",
    "\n",
    "import sparkdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame containing all the pictures\n",
    "\n",
    "### Read images and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads recursively all images in the specified directory, put in a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'SAMPLE2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Option1: Get local data\n",
    "\n",
    "data_path = os.path.join(\"../data/fruits-360\", PREFIX)\n",
    "    \n",
    "# Option2: Get data from s3\n",
    "\n",
    "# bucket='ocfruitpictures'\n",
    "# folder = PREFIX\n",
    "# data_path = 's3n://{}/{}'.format(bucket, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads all images contained in the directory\n",
    "\n",
    "images_df = ImageSchema.readImages(data_path,\n",
    "                                   recursive=True).repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7083464"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Return a JavaRDD of Object by unpickling\n",
    "It will convert each Python object into Java object by Pyrolite, whenever the\n",
    "RDD is serialized in batch or not.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "\n",
    "# Function to convert python object to Java objects\n",
    "def _to_java_object_rdd(rdd):  \n",
    "    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
    "\n",
    "# Convert DataFrame to an RDD \n",
    "JavaObj = _to_java_object_rdd(images_df.rdd)\n",
    "\n",
    "# Estimate size in bytes\n",
    "nbytes = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)\n",
    "nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the content of the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a Spark DataFrame containing all the images, each as one row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample picture\n",
    "\n",
    "Extract first picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract first row of the DataFrame\n",
    "# row0 = images_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row0.asDict()['image']['mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform the row in a dict, and turn the data in a 1D np.array\n",
    "# mat0 = np.array(row0.asDict()['image']['data'])\n",
    "# # reshape the 1D vector into a 3 channel, 2D np.array of pixels\n",
    "# mat0 = mat0.reshape(100, 100, 3)[:, :, ::-1] # reverse BGR to RGB\n",
    "# mat0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# # Display sample image\n",
    "# Image.fromarray(mat0, 'RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction (Transfer Learning) using Sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import DeepImageFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of the featurizer\n",
    "feat = DeepImageFeaturizer(inputCol=\"image\",\n",
    "                           outputCol=\"image_features\",\n",
    "                           modelName=\"ResNet50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiation of a sparkdl pipeline to process the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe = Pipeline(stages=[feat])\n",
    "extractor = pipe.fit(images_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_features_df = extractor.transform(images_df)\n",
    "# ext_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare the size of the Spark DataFrame (prior action)\n",
    "# # and that of a Pandas DataFrame\n",
    "\n",
    "# import sys\n",
    "# print(sys.getsizeof(ext_features_df),\n",
    "#       sys.getsizeof(ext_features_df.toPandas()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# instantiate Spark PCA model\n",
    "pca = PCA(k=8,\n",
    "          inputCol=\"image_features\",\n",
    "          outputCol=\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the extracted features\n",
    "model = pca.fit(ext_features_df.select('image_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative explained variance\n",
    "cumValues = model.explainedVariance.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show the scree plot\n",
    "plt.rcParams['figure.facecolor']='w'\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(range(1,9), cumValues, color='r',\n",
    "         marker = 'o', linestyle='--')\n",
    "plt.title('Scree plot')\n",
    "plt.xlabel('Number of first components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the projection of the extracted features using PCA\n",
    "\n",
    "pca_feat_df = model.transform(ext_features_df)\n",
    "# pca_feat_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the class of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get class of the fruits\n",
    "\n",
    "orig_col = pca_feat_df['image']['origin']\n",
    "split_col = pyspark.sql.functions.split(orig_col,\n",
    "                                        PREFIX+'/')\n",
    "\n",
    "# add a new \"label\" column\n",
    "df_ = pca_feat_df.withColumn('labels',\n",
    "                             split_col.getItem(1))\n",
    "split_col = pyspark.sql.functions.split(df_['labels'],\n",
    "                                        '/')\n",
    "df_ = df_.withColumn('labels',\n",
    "                     split_col.getItem(0))\n",
    "\n",
    "df_ = df_.withColumnRenamed(\"image\", \"path\")\n",
    "\n",
    "# df_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = df_.select('path','pca_features','labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write final DataFrame in parquet format in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.csv as pv\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# # results_pd = results_df.toPandas()\n",
    "# pq.write_table(results_df, 'test1.parquet') # Index(['path', 'pca_features', 'labels'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\") # gzip, lzo or lz4\n",
    "# spark.conf.set(\"spark.sql.parquet.compression.codec\", \"uncompressed\")\n",
    "# spark.sql(\"SET parquet.compression=SNAPPY\")\n",
    "# spark.sql(\"SET spark.sql.parquet.compression.codec=snappy\")\n",
    "# df_.write.parquet(\"p0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_pd = results_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MARCHE !!!\n",
    "# f = open(\"/home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/mon_texte2.txt\", 'w+')\n",
    "# f.write('contenu')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MARCHE PAS!!!\n",
    "# path = \"/home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/P0.json\"\n",
    "# results_df.write.json('truc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MARCHE\n",
    "results_df.write.mode('overwrite').parquet(\"s3://ocfruitpictures/RESULTS_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MARCHE PAS!!!\n",
    "# path = \"file:///home/maryse/PARTAGE/FORMATION/OCR_DS/PROJET8/Pultimate.parquet\"\n",
    "# results_df.write.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MARCHE!!!\n",
    "# path = \"file:///home/maryse/Pultimate.parquet\"\n",
    "# results_df.write.parquet(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p8_venv",
   "language": "python",
   "name": "p8_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
